# Ch.6 Partitioning

## Partitioning and Replication

* Partitioning is usually combined with replication. Each parition has leader and followers, on different nodes

## Partitioning of key-value data

* Goal: evenly spread the data and query load across nodes
  * Skewed(倾斜), causes hot spot
  * To avoid hot spot, the simplest approch is to assign data randomly, but time-consuming when read

### Partitioning by key range

* The partition boundaries need to adapt to the data, in order to distribute the data evenly.
* boundaries can be managed manually or automatically, more in "Rebalancing Paritions"
* Within each partition, data can be sorted by key
* Downside: causing hot spots for some use case. e.g. partitioned by time, then new writes will all go to the same partition, while other partition idle.
* Solution: Use another field as the first element of key. e.g. type, if multiple types of records are written to db simultaneously, writes will be distributed to diff partition.

### Partitioning by hash of key

* Good hash function makes data uniformly distributed
* Downside: Lose a nice property of key-range partitioning, for range-queries, since keys are scattered across all partitions
  * MongoDB: send range query to all partitions.
  * Riak, Couchbase, Voldemort: range query on pk is not supported
* Cassandra: compound primary key consisting several columns. Only the first part of key is hashed to determine paritition, other columns are used as concatenated index for sorting data in SSTables. So, cannot range-query on the 1st columns, but can on othe key columns.
* The concatenated index approach is a good model for one-to-many relationships. e.g. pk is (user-id, timestamp), hash user-id to diff parittions, but for each use, the records are adjacent.

### Skewed workloads and releaving hot spots

* Hashing helps distributed data, but can't avoid hot spot entirly.
* In extreme case: all read and write to the same key, e.g. celebrity id or hot topics
* Solution: if one key is hot, split it to 100 keys by giving random prefix/suffix, e.g.(00~99), so they will be distributed to diff partitions
  * Downside: additional effort, have to read from all these partitions.
  * Also, need metadata to track which key is split
* Today, most systems cannot auto detect and handle skewed workloads. It need app dev to consider the tradeoffs

## Partitioning and secondary indexes

* Challenge: secondary indexes don't map neatly to partitions

### Partitioning secondary indexes by document

* local index: each partition maintain its own secondary indexes, covering only the data in that partition
* scatter/gather: have to read from all partitions. high latency, expensive
* widely used: MongoDB, Riak, Cassandra, ElasticSearch, SolrCloud, VoltDB

### Partitioning secondary indexes by term

* global index: covers all data in all partitions
* partition diff term value on diff partitions
* Faster to read, only read index on 1 partition
* Complex to write, data and index could be on diff partitions
* Hard to main syncronized update to secondary index, often async in practice

## Rebalancing partitions

* Over time, changes in db:
  * query thruput increase -> need more cpu
  * data size increase -> need more disk and RAM
  * machine fails -> need new machine
* All above case need to move data, aka rebalancing
* Requirements:
  * more balanced after rebalancing
  * db keep working while rebalancing
  * avoid unnecessary data movement

### Strategy for rebalancing

* How NOT to do it: hash mod n ?
  * hash mod n:  e.g.  partition number = hash(key) mod 10
  * If n change, rebalancing is expensive
* Fixed number of partitions
  * 创建远超实际节点数的分区数, 并且通常后续不能修改 e.g. 10 nodes, 1000 partitions, each node takes about 100 partitions
  * 如果有新节点加入，可以从每个旧节点上匀一个分区出来
  * If partition is large, rebalancing and recovering are expensive
  * If partitions are too small, extra overhead
  * Choosing right number of partitions are difficult if data size is highly variable
* Dynamic partitioning
  * 如果一个分区变得太大，就分裂成两个。如果一个分区随后变得太小，就与邻居合并
  * 初始时
    * 从单个分区开始，所有IO集中于单个分区，直到达到分裂阈值
    * 在知道数据分布的情况下，HBase, MongoDB 支持预分裂(pre-splitting)
* Partitioning proportionally to nodes  
  * 每个node分区数固定，比如Cassandra默认256个
  * 当新节点加入，随机选n(比如256)个分区分走一半数据。

### Operations: Auto or manual rebalancing

* Auto
  * more conveniet, with less operation
  * unpredict result, risk of oveloading network
  * combined with auto failure detection, if a node is overloaded and response slow, other node thought it is dead, and trigger the rebalancing. 更加overload the whole system and potentially causing cascading failure
* Manual
  * safer, prevent operational surprise
  * DB can generate suggested partition assignment, let admin admit

## Request Routing



### Parallel Query Execution

## Summary
