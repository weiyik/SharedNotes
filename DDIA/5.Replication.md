# Part II. Distributed Data

Why?

* Scalability
* Fault tolerance/High availability
* Latency

Scaling to Higher Load

* Vertically scaling / scaling up
  * shared-memory architecture
    * cost grows faster than linearly
    * fault tolerance and single geographic location
  * shared-disk architecture
    * fast network, usually used in data warehousing workloads
    * contention and overhead of locking limit scalability
* Shared-Nothing Arthictectures
  * node, software coordination, convventional network
  * Advantages
    * best price/performance ration
    * distribute data across multiple geographic regions, latency, HA
  * Complexity
* Replication vs Partitionning
  * Replication
    * same data on diff nodes, redundancy, improve performance
  * Partitionning
    * subsets of data on diff nodes

# Chapter 5. Replication

## Intro

* Difficulty: Handling changes to replicated data
* 3 popular algo
  * single-leader
  * multi-leader
  * leaderless
* trade-offs, e.g.
  * whether to use synchronous or asynchronous replication
  * how to handle failed replicas

## Leaders and Followers

* Intro
  * Q: how the data ends up to be on the all replicas?
  * The most common soltion: leader-based replication, aka, active/passive or master-slave replication
    1. One replica is leader. Clients write to leader
    2. Other replicas are followers. When leader have new data, it sends data changes to all followers as *replication* log or *change stream*.
    3. Clients read from any replica, but only write to the leader
  * Used on: 
    * RDBMS: PostgreSQL, MySQL, ... 
    * NoSQL: MongoDB, Rethink, Espresso
    * Message brokers: Kafka, RabbitMQ HA Queue
* Syncronous vs Asynchronous Replication
  * always configurable in RDBMS, and hardcoded in other systems
  * advantage of syncronous replication: guaranteed to have up-to-date copy on followers, if leader suddenly fails
  * disadvantage of syncronous replication: if sync follower no respond, have to block all writes, and wait until the sync replica is available again.
  * Semi-synchronous Config. In practice, if enable sync replication in DBMS, it usually means one of the followers is sync, and others are async. If sync one are down or slow, one of the async can become sync. At least 2 nodes have up-to-date copy of data
* Setting up new followers
  1. take snapshot, copy snap shot to new follower
  2. follower request data changes after the snapshot, process them. Then, we call it caught up.
* Node Outage
  * follower failure: catch-up recovery
  * leader failure: failover (节点切换)
    * Steps of failover
      1. 确认leader失效: 心跳，超时
      2. Choosing new leader: 1) election 2) assigned by controller
      3. Reconfigure the system to use the new leader
    * Things can go wrong
      * if async replication, new leader may not have the latest data. If older leader comes back, 数据可能有冲突，这部分数据可能被丢弃。违背持久化的承诺，duribility
      * 当多系统协作时，丢弃数据是危险的。可能导致私有数据泄露。
      * split brain: 两个节点都认为自己是leader. 数据可能被破坏
      * proper timeout before failover? longer downtime v.s. unnecessary failovers
* Implementation of Replication Logs
  * Statement-based Replication
    * leader log the request, and sends them to followers. Followers execute that request.
    * Problems:
      * nonderterministic function, like rand()
      * auto-incrementing columns, or request depending on current data in db, order matters.
      * Used on: MySQL 5.1-
  * Write-ahead Log (WAL) Shipping
    * log is an append-only sequence of bytes
    * leader send the log to follower, and follower process the log to build exact the same data
    * Disvantage: low level, coupled to storage engine. Weak compatibility btw diff versions of leader and follower
    * Used on: PostgreSQL, Oracle
  * Logical(Row-based) Log Replication
    * for RDBMS, logical log are records describing writes to db tables at row granularity
    * a transaction generate several records, followed by a record indicating the transactionwas commited. MySQL's binlog
    * Advantages:
      * decoupled from storage engine. easier to keep backward compatibility.
      * easier to parsed by external apps.
  * Trigger-based replication
    * flexibility, triggers and stored procedures
  
## Problems with Replication Lag

* Reading Your Own Writes
  * Problem desc: user write to leader; user read from follower before data is synced; user seems to lose the data he just submitted.
  * Possible solutions:
    1. read from leader, for sth could be modified, for example, user's own profile. Still read other data from followers, e.g. others' profiles
    2. track the last update time of client. For one minute after the last update, read from reader. Or, montor the replication lag on followers, and prevent querying from followers falls one minte behind.
    3. client send request with its last udpate time, system can forward this request to newer replica if current replica is old. BUT: clock sync is a problem
  * Problems: 
    * replica across multiple datacenters, latency
    * multiple client for same user, (cross-device read-after-write consistency)
* Monotonic Reads
  * Problem desc: A user first reads from a fresh replica, then from a stale replica. Time appeas to go backwards.
  * one solution: each user always read from the same replica
* Consistent Prefix Reads
  * Problem desc: If some partition are replicated slower than others, an observer may see the answer before the question. (answer depends on question, update depends on insert)
  * One solution is to make usre writes with causal relation are written to the same partition, but in some apps this is not efficient.
* Solutions for Replication Lag
  * Think, how your app behave if lag increased to minutes or hours?
    * If there's problem, then you need stronger guuuarantee
  * Transaction. -> Chapter 7 and 9

## Multi-Leader Replication

* Use cases for multi-leader replication
  * Multiple-DataCenter Operation
    * Compare single-leader vs multi-leader
      * Performance: (write) single-leader: high latency, multi-leader: write to leader in local datacenter and async copy to other data center
      * Tolerance of datacenter outages: single-leader: must failover to node in another datacenter, multi-leader: each datacenter can continue working
      * Tolerance of network problems: single-leader: usually with sync replication, sensitive to inter-datacenter link. multi-leader: with async replication can tolerate network problems better.
  * Clients with offline operation
    * For example, calendar app on diff devices, each device act like a data center, and has local db. replication lag is very long.
    * CouchDB
  * Collaborating editing
    * Many users are editing same doc at the same time. User's change applied to local replica first and asyncly replicated to server and other user.
    * Conflict. Lock. small lock, even no lock.
* Handing write conflicts
  * Problem: A write conflict caused by 2 leaders concurrently updteing the same record
  * Sync vs Async conflict detection
    * Async: in above case, the conflict is conly detected asyncly in later time. Then it's too late to ask user to resolve the conflict.
    * Sync: replicated the change to all replica before write succ. Lose the advantage of multi-leader. Single-leader would be more suitable.
  * Conflict avoidance
    * If app can ensure all write for particular record go thru the same leader, then conflict cannot occur.
  * Converging toward a consistent state
    * In multi-leader config, writes ordering is not defined. So system don't know which write happens last.
    * Achiving convergent conflict resolution:
      1. Last write win(LWW): Give each write a uniq ID, pick the write with highest ID. Data loss
      2. Give each replica uniq ID. higher-numbered replica take precedence over writes to lower replica. Data loss
      3. Somehow merge the value together, keep them both
      4. record conflict, and resolve by app code later
  * custom conflict resolution logic
    * resolving conflict by application code.
    * On write
      * As long as db detects conflict in log of replicated change, it call conflict handler
    * On read
      * Stored all velue when conflict. When client read, return multiple version oif data, let user resolve and write result back to db. CouchDB
  * What is a conflict?
    * Some are obvious, like modify same record concurrently
    * Some are subtle to detect, for example, book meeting room, what if 2 bookings are made on diff leaders, with the same meeting room and time slot.
    * More in Chapter 7, 12
* Multi-leader replication topologies
  * Cicular, MySQL default
  * Star
  * All-to-all, common

## Leaderless Replication

* (Amazon) Dynamo-style: Riak, Cassandra, Voldemort
* Writing to the DB when a node is down
  * With leader based config, have to failover. With leader-less, no need. Say 3 node in cluter, client send write to all 3 node. If success on 2 of 3, then write is success.
  * Read from multiple nodes to get the newer vertion of data.
  * Read repair and anti-entropy
    * After an unavailable node comes back online, how does it catch up?
    * Read repair
      * Client read from serveral nodes, detect stale value, write newer value to the "stale" replica
    * Anti-entropy process
      * db background process constantly looks for diff among replica. Replcation lag
  * Quoroms for reading and writing
    * $n$ replica, write need to be confirmed by $w$ nodes, read at least $r$ nodes
    * Normally, $w + r > n$, so $w$ and $r$ should have overlap
    * in Dynamo-style db, normally $n$ odd number, typically 3 or 5. $w=r=\lceil(n+1)/2\rceil$
    * If workload with few writes and many reads, $w=n$, $r=1$ for faster read.
* Limitation of Quorum Consistency
  * For low latency and high availability, we can also set $w+r<n$. In this case, user may read stale value.
  * Even with $w+r>n$, stale value could also be returned. possible scenarios
  * No guarentee of read-your-writes, monotonic reads, consistent prefix reads. Stronger guarentee need transactions or consensus
  * Monitoring Staleness
    * If some nodes falls behind significantly
* Sloppy Quorums and Hinted Handoff
  * Sloppy Quorums
    * In a large cluster(# of node >> n), client can connect to some node, but not the nodes needed for n. Write and read still require w and r successful response, but those my include node that are not among the designated n "home" nodes for a value.
    * Analogy: if you lock yourself out of your house, you may stay at your neighbor's house temporarily
    * Once the origial node get back alive, the data should be handover
  * Multi-datacenter operation
* Detecting Concurrent Writes
  * Problem:
    * Concurrent writes in a Dynamo-style datastore, there's no well-defined ordering
  * Last write wins(discarding concurrent writes)
    * How to defined the order? The natual oder cannot be defined with concurrent writes
    * we can force arbitrary order, like timestamp
    * LWW achieves eventual convergence, but at the cost of durability
    * The only safe way using LWW is to ensure that a key is only written once and immutable, thus avoiding concurrent updates to the same key.
  * The "happens-before" relationship and concurrency
  * Capturing the happens-before relationship
    * server maintains version number for every key, +1 for every write
    * when client reads, return all value that have not been overwritten, as well as the latest version number
    * when client write, it read first, and the write must include the version number of prior read, and merge together all values in prior read.
    * When server receive write with a particular version number, it can overwrite all values with that version number or below
  * Merging concurrently writing values
    * For simple use case like adding to cart, we can simply merge the sets
    * But what if remove items from cart? Use deletion marker, aka tombstone
    * data structure that merge automatically, like CRDT in Riak
  * Version vectors
    * When it comes to multi replica, version number should become vector
    * need to use version number per replica, per key

## Summary

* Purposes of replication:
  * High availability: system running when some servers and data centers down
  * Disconnected operation: allow app work when network interrupted
  * Latency: data close to user
  * Scalability: high volumn
* Main approaches:
  * Single-leader replication
    * Write to leader. Read from followers, value might be stale
  * Multi-leader replication
    * Write to one of the leaders.
  * Leaderless replication
    * Write to several nodes, read from several nodes in parallel. w+r>n
* Consistency models, handling replication lag:
  * Read-after-write consistency
    * users see data write by themselves
  * Monotonic reads
    * after users see data at one point in time, they shouldn't later see the data from earlier point in time
  * Consistent prefix reads
    * Users should see the data in a state that makes causal sense, 因果关系
* Concurrency issue
  * conflict
  