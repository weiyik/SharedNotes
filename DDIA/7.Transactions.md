# Transactions

## The Slippery Concept of a Transaction

### The Meaning of ACID

* BASE(not ACID): Basiscally Available, Soft state, Eventually consistency
* Atomicity
  * The ability to abort a transaction on error and have all wwrite from that transaction discarded. abortability
* Consistency
  * Some statement of you data that must always be true. Application's responsibility
  * C doen't really belongs to ACID
* Isolation
  * concurrently executing transactions are isolcated from each other.
  * serializability: db ensures that when all transactions committed, the result is the same as if they had run serially.
* Durability
  * promise that once a transaction has committed successfully, any data it has written will not be forgotten.

### Single-Object and Multi-Object Operations

* Single-object Writes
  * For a single object, provide atomicity and isolation.
  * Atomicity can be implemented by log for crach recovery
  * Isolcation can be implemented by lock ok each object
  * Also, complex atomic operation, like increment operation(原子自增操作), to avoid read-modify-write
  * compare-and-set operation, allows a write only if the value hasn't been concurrently changed by someone else
* The need for multi-object transactions
  * RDBMS/Graph DB: foreign key/edges between vertices
  * document db: update several documents in one go
  * db with secondary indexes. Index are also different objects, without transaction isolation, second index may not be up to date when read.
* Handling errors and aborts   
  * Key feature of transaction: it can be aborted and safely retried.
  * problems of retrying aborted transaction:
    * If transactoin actually success, but network failed when telling the client, client think it failed and retry. causing duplicated data.
    * If error is due to overload, retrying is making it worse. So set the limit number of retries, use exponential backoff(?)
    * For permanent error, retrying is pointless
    * If transaction has side effect out of db, like sending out an email. If you want to make sure that several diff systems either commit or abort together, 2PC can help, 2-phase commit
    * If retrying still fails, data lost 

## Weak Isolation Levels

* serializable isolation: db guarantee that transactions have the same effect as if they ran serially
* serializable isolation has performance cost. so weker isolation level, to prevent some concurrent issue

### Read Commited

* Guarantees:
  1. When reading, only see commited data. no dirty reads
  2. When writing, only overwrite commited data. no dirty writes
* No dirty reads
  * If a transaction need to update multiple objects, dirty read means another transaction see db paritially updated.
  * If a transaction aborts, roll back the writes. If dirty read, it could read some data is later rolled back / never actually commited
* No dirty writes
  * When two transaction concurrently try to update the same object
  * If later write overwrite an uncommitted value, it's dirty writes. 
  * A common way to prevent dirty write is, delaying the second write until the first write's transaction has commited or aborted
  * Preventing dirty write can avoid some concurrent problems:
    * If transactions update multiple objects, dirty write can lead to unpreidicted bad outcome
* Implementing read commited
  * row-level locks, prevent dirty writes
  * To prevent dirty reads, locks are too expensive. db keeps both old and new value. Before transaction commited, read the old one.

### Snapshot Isolation and Repeatable Read

* Read skew / nonrepeatable read
  * e.g. With RC, transaction-A will read 2 objects.
  * While transaction-A just have read the first object, another transaction-B modify the data and commited.
  * Then transaction-A read the 2nd object. 
  * Since the first object is read from the older version, and the second from a newer version. They could seems to be inconsistent, temporarily.
* Some situations cannot tolerate tempory inconsistency:
  * Backups: take long to make copy of entire db. While backing up, new data are written to db, then the backup data could be inconsistent. If db recover with inconsistent data, then it cause permanent inconsistency. 
  * Analytic queries and integrity checks: probably scan over large parts of data and take long time to complete
* Implementation of snapshot isolation
  * Use write locks to prevent dirty writes
  * Reads don't require lock. readers never block writers, and writers neber block readers
  * MVCC(Multi-Version Concurrency Control): keep several versions of an object 
  * MVCC Implementation in PostgreSQL:
    * When a transaction started, it's given a uniq, increasing transaction id(txid).
    * Each row in table has `created_by` field and `deleted_by` field containing txid
    * Tag the data it writes with txid. When update data, create a new row and "delete" the precious version row
* Visiblity rules for observing a consistent snapshot
  * When a transaction reads from db, use txid to deside which object is visible
  * Rules to present a consistent snopshot of db:
    1. ignore data written by ongoing transactions
    2. ignore the writes made by aborted transactions
    3. ignore writes made by transaction with a later txid
    4. all other writes are visible
* Indexes and snapshot isolation
  * One option is to have the index simply point to all versions of an object and require an index query to filter out invisible objects for current transaction.
  * Append-only B-tree
* Repeatable read and naming confusion
  * In Oracle, snapshot isolation level is called *serializable*
  * In ProstgreSQL and MySQL, snapshot isolation level is called *repeatable read*
  * IBM DB2 uses "repeatable read" to refer to serializability

### Preventing Lost Updates

* Lost update problem can occur when *read-modify-write*
* Atomic write operation
  * usually best choice
  * e.g. concurrently safe in most db, `update counters set value = value + 1 where key = 'foo'`
  * MongoDB provide atomic operations for making local changes to part of JSON
  * Redis provide atomic operation for modifying data structure like priority queue
  * Implementation: usually exclusive lock on object, other transactions cannot read while it's locked. *cursor stability*
  * ORM potentially performes unsafe read-modify-writes
* Explicit locking
  * Explicitly lock object, for example, `SELECT ... FOR UPDATE` indicate db should lock returned rows
  * This appoach works, but easy to forget in many application.
* Automatically detecting lost update
  * Allow execute in parallel. If a transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle
  * It's automatically in DB, less error-prone for app
  * DB can perform this check with snapshot isolation.
    * PostgreSQL's repeatable read, Oracle's serializable, and SQL Server's snapshot isolation automatically detect lost update
    * BUT, MySQL/InnoDB's repeatable read don't detect lost updates
* Compare-and-set
  * In db without transaction, atomic compare-and-set operation allow an update happen only if the value has not changed since you last read it.
  * If not match, retry read-modify-write cycle
* Confilict resolution and replication
  * In replicated db, especially with multi-leader and leaderless replication, writes happen concurrently on replicated asyncly. So locks and cas do not work.
  * Concurrent writes create several conflicting versions of a value(siblings) and use app code or special data structure to resolve
  * LLW(Last Write Wins) is prone to lost updates, but widely used as defult in many replicated db

### Write Skew and Phantoms

* Characterizing write skew
  * It's neither dirty write nor lost update, because 2 transactions are updating 2 diff objects. It's less obvious that a conflit occured here, but it's a race condition.
  * Write skew can occur if 2 transactions read the same objects and then update diff objects
* More examples to write skew
  * Meeting room booking system
    * With snapshot isolation, below steps are not safe
        1. begin transaction
        2. check availability for room-A at 10:00
        3. if ok, insert my booking
        4. commit
    * To avoid conflicts, it need serializable isolation
  * Multiplayer game
    * lock may prevent lost update (2 players cannot move the same figure simutaneously)
    * but, lock on figure doesn't prevent players moving figure to the same position, or violation of other constraints
  * Claiming a username
    * 2 user try to create the same username
    * unique constraints is a simple solution here
  * Preventing double-spending
* Phantoms causing write skew
  * All above examples follow a similar pattern:
    1. select query check wheter some requirements is satisfied.
    2. depending on the result of 1st query, app deside how to continue
    3. if app decide to go ahead, it write to db and commit. The effect of the write change the precondition of the decisioon of step 2
  * *Phantom*: This effect, where a write in one transaction changes the result of a search query in another transaction
  * Snapshot isolation can prevent phantoms in read-only queries, but in read-write transaction, phantoms can lead to write skew.
* Matirializing conflicts
  * If the problem of phantoms is that, there's no object to lock on, perhaps, we can introduce some object to lock.
  * e.g. for meeting room booking, a ttable of time slots and rooms
  * *Materializing conflicts*: turn a phantom into a lock conflict
  * However, it could be hard and error-prone. not recommended unless no althernative is possible
  * serializable isolation level is much preferable

## Serializability

Serializable isolation is the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency. Thus, the database guarantees that if the transactions behave correctly when run individually, they continue to be correct when run concurrently—in other words, the database prevents all possible race conditions.

### Actual Serial Execution

* Intro
  * Simply execute only one transaction at a time, in serial order, on a single thread.
  * avoid coordination overhead of locking
  * Why multi-thread to single-thread?
    * RAM become cheaper enough to keep all active data in memory
    * OLTP are short and make small number of read/writes. While OLAP can run on snapshot outside of serial execution loop.
  * Implemented in VoltDB/H-Store, Redis, Datomic
  * its thruput is limited to single CPU core
* Encapsulating Transactions in stored precudure
* Pros and cons of stored procedure
* Partitioning
* Summary of serial execution

### Two-Phase Locking (2PL)

### Serializable Snapshots Isolation (SSI)

## Summary
