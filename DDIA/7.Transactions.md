# Transactions

## The Slippery Concepy of a Transaction

### The Meaning of ACID

### Single-Object and Multi-Object Operations

* Single-object Writes
  * For a single object, provide atomicity and isolation.
  * Atomicity can be implemented by log for crach recovery
  * Isolcation can be implemented by lock ok each object
  * Also, complex atomic operation, like increment operation(原子自增操作), to avoid read-modify-write
  * compare-and-set operation, allows a write only if the value hasn't been concurrently changed by someone else
* The need for multi-object transactions
  * RDBMS/Graph DB: foreign key/edges between vertices
  * document db: update several documents in one go
  * db with secondary indexes. Index are also different objects, without transaction isolation, second index may not be up to date when read.
* Handling errors and aborts   
  * Key feature of transaction: it can be aborted and safely retried.
  * problems of retrying aborted transaction:
    * If transactoin actually success, but network failed when telling the client, client think it failed and retry. causing duplicated data.
    * If error is due to overload, retrying is making it worse. So set the limit number of retries, use exponential backoff(?)
    * For permanent error, retrying is pointless
    * If transaction has side effect out of db, like sending out an email. If you want to make sure that several diff systems either commit or abort together, 2PC can help, 2-phase commit
    * If retrying still fails, data lost 

## Weak Isolation Levels

* serializable isolation: db guarantee that transactions have the same effect as if they ran serially
* serializable isolation has performance cost. so weker isolation level, to prevent some concurrent issue

### Read Commited

* Guarantees:
  1. When reading, only see commited data. no dirty reads
  2. When writing, only overwrite commited data. no dirty writes
* No dirty reads
  * If a transaction need to update multiple objects, dirty read means another transaction see db paritially updated.
  * If a transaction aborts, roll back the writes. If dirty read, it could read some data is later rolled back / never actually commited
* No dirty writes
  * When two transaction concurrently try to update the same object
  * If later write overwrite an uncommitted value, it's dirty writes. 
  * A common way to prevent dirty write is, delaying the second write until the first write's transaction has commited or aborted
  * Preventing dirty write can avoid some concurrent problems:
    * If transactions update multiple objects, dirty write can lead to unpreidicted bad outcome
* Implementing read commited
  * row-level locks, prevent dirty writes
  * To prevent dirty reads, locks are too expensive. db keeps both old and new value. Before transaction commited, read the old one.

### Snapshot Isolation and Repeatable Read

* Read skew / nonrepeatable read
  * e.g. With RC, transaction-A will read 2 objects.
  * While transaction-A just have read the first object, another transaction-B modify the data and commited.
  * Then transaction-A read the 2nd object. 
  * Since the first object is read from the older version, and the second from a newer version. They could seems to be inconsistent, temporarily.
* Some situations cannot tolerate tempory inconsistency:
  * Backups: take long to make copy of entire db. While backing up, new data are written to db, then the backup data could be inconsistent. If db recover with inconsistent data, then it cause permanent inconsistency. 
  * Analytic queries and integrity checks: probably scan over large parts of data and take long time to complete
* Implementation of snapshot isolation
  * Use write locks to prevent dirty writes
  * Reads don't require lock. readers never block writers, and writers neber block readers
  * MVCC(Multi-Version Concurrency Control): keep several versions of an object 
  * MVCC Implementation in PostgreSQL:
    * When a transaction started, it's given a uniq, increasing transaction id(txid).
    * Each row in table has `created_by` field and `deleted_by` field containing txid
    * Tag the data it writes with txid. When update data, create a new row and "delete" the precious version row
* Visiblity rules for observing a consistent snapshot
  * When a transaction reads from db, use txid to deside which object is visible
  * Rules to present a consistent snopshot of db:
    1. ignore data written by ongoing transactions
    2. ignore the writes made by aborted transactions
    3. ignore writes made by transaction with a later txid
    4. all other writes are visible
* Indexes and snapshot isolation
  * One option is to have the index simply point to all versions of an object and require an index query to filter out invisible objects for current transaction.
  * Append-only B-tree
* Repeatable read and naming confusion
  * In Oracle, snapshot isolation level is called *serializable*
  * In ProstgreSQL and MySQL, snapshot isolation level is called *repeatable read*
  * IBM DB2 uses "repeatable read" to refer to serializability

### Preventing Lost Updates

* Lost update problem can occur when *read-modify-write*
* Atomic write operation
  * usually best choice
  * e.g. concurrently safe in most db, `update counters set value = value + 1 where key = 'foo'`
  * MongoDB provide atomic operations for making local changes to part of JSON
  * Redis provide atomic operation for modifying data structure like priority queue
  * Implementation: usually exclusive lock on object, other transactions cannot read while it's locked. *cursor stability*
  * ORM potentially performes unsafe read-modify-writes
* Explicit locking
  * Explicitly lock object, for example, `SELECT ... FOR UPDATE` indicate db should lock returned rows
  * This appoach works, but easy to forget in many application.
* Automatically detecting lost update
  * Allow execute in parallel. If a transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle
  * It's automatically in DB, less error-prone for app
  * DB can perform this check with snapshot isolation.
    * PostgreSQL's repeatable read, Oracle's serializable, and SQL Server's snapshot isolation automatically detect lost update
    * BUT, MySQL/InnoDB's repeatable read don't detect lost updates
* Compare-and-set
  * In db without transaction, atomic compare-and-set operation allow an update happen only if the value has not changed since you last read it.
  * If not match, retry read-modify-write cycle
* Confilict resolution and replication
  * In replicated db, especially with multi-leader and leaderless replication, writes happen concurrently on replicated asyncly. So locks and cas do not work.
  * Concurrent writes create several conflicting versions of a value(siblings) and use app code or special data structure to resolve
  * LLW(Last Write Wins) is prone to lost updates, but widely used as defult in many replicated db

### Write Skew and Phantoms

## Serializability

### Actual Serial Execution

### Two-Phase Locking (2PL)

### Serializable Snapshots Isolation (SSI)

## Summary
