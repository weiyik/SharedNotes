# Architecture

* Big Picture View
  * K8s is an orchestrator of micro services apps
  * Analogy: football team
    * players have diff skills
    * coach give every one diff roles and responsibility, to form a team
    * in conctantly changing situation, coach can replace players
    * K8s is like the coach in a footable team, organizing everything into a useful app
  * cluster, containing masters and nodes
  * masters: Control plane
  * nodes: do the work and report back to masters
  * deployment - pod - app
    * defined in YAML file
* Masters
  * aka. head nodes, control plane
  * Multi-master high available control plane, putting them on diff failure domains
  * Number of masters
    * 3 is good, 5 is ok, too many master nodes may cause difficulties to achieve consensus
    * Even number is a bad idea, split brain -> deadlock
  * Roles
    * 1 Leader
    * Others are followers
    * Election, if leader is down
  * only Linux
  * Hosted K8s, cloud service provider run k8s masters for you. master nodes are invisible for users, and out of control. Expose highy available API endpoint
  * Don't run user apps on masters
  * Components
    * kube-apiserver
      * front-end to the control plane
      * Exposes REST API
      * JSON/YAML
    * Cluster store (KV)
      * persists cluster state and config
      * based on etcd
      * Performance is critical
      * Have recovery plans in place
    * Kube-controller-manager
      * Controller of controllers
        * Node controller
        * Deployment controller
        * Endpoints/EndpointSlice controller
      * Watch loop
      * Reconcile observed state with desired state
    * Kube-scheduler
      * Watch API Server for new work task
      * Assigns work to cluster nodes
        * Affinity关联/Anti-affinity反关联
        * Constraints
        * Taints污点
        * Resources
    * kubctl -> apiserver (authN + authZ) -> cluster store -> scheduler -> controllers (monitoring)
* Nodes
  * Components
    * Kubelet
      * Main k8s agent, run on every node. We often use the words interchangebally, node <-> kubelet
      * Register node with cluster
      * Watch API  server for work tasks (Pods)
      * Executes Pods
      * Reports back to masters
    * Container Runtime
      * Can be Docker
      * Pluggable: CRI (Container Runtime Interface)
        * Docker, containerd, CRI-O, Kata...
      * Low-level container intelligence
    * Kube-proxy, network brain
      * Networking component
      * Pod IP addresses, 1 IP per Pod
      * Basic load-balancing
  * Virtual Kubelet: Nodeless Kubernetes
    * pods run on cloud's hosted container back-end
  * Linux, Windows
* The Declarative Model and Desired State
  * Declarative Model
    * Describe what you want (desired state) in a manifest file
    * post the manifest to k8s api, then k8s takes all the work, to keep the desired state are implemented
    * For example, if client declare to have 3 instances of a web service, k8s implement and monitor them.
    * If observed state deverges from the desired state, k8s will work on that until they match
* Pods
  * Atomic units of scheduling/deploying
    * VM, in VM world
    * Container, in container world
    * Pod, in K8s
      * Yes, K8s orchestrate containers, but containers must always run inside of pods
  * pod is a shared execution environment
    * ip addr and network ports
    * file system
    * volume, memory
    * If you have multiple containers in 1 pod, they all share same environemnt/resources, e.g. same ip, then each container have diff port number
  * Coupling pods
    * Tightly coupled (2 or more container in 1 pod)
      * 2 container must share volumes, memory, etc.
    * Loosely coupled (containers in diff pods)
      * 2 containers don't have to share resources.
  * Scaling
    * Scale up -> add more pods, NOT add more containers in 1 pod
  * Example: Service mesh
    * Injecting a mesh container in to an application pod for providing enhanced services
  * Atomic
    * pod deployment is an atomic operation. the pod show up and running service only when all containers in it are up and running
    * containers in a pod are always schedule to a single node. They need share environment/resources
  * Mortal
    * pending -> running -> succeeded/failed
    * no such thing like coming back to live
  * Why pod over bare containers
    * annotations
    * labels
    * policies
    * resources constraints and requirements
    * co-scheduling containers
* Services
  * pods' IPs are unreliable, when deploy new pods, scale up/down, rolling release
  * stable name and ip to service,   load balancing to app pods
  * Labels
    * simple, powerful, flexible
    * Service use labels to find the pods and include into the load balancing group
  * features:
    * Services only send traffic to healthy pods
    * Can do session affinity
    * can send traffic to endpoints outside the cluster
    * can do tcp and udp
* Deployments
  * Deployment Controller/Reconciliation loop
    * Watch API Server for new Deployments
    * Implements them
    * Constantly compares observed state with desired state
  * Deploy: manages updates,rollbacks
    * Replica set: Replica count, self-healing, previous versions
      * Pod: labels, annotations, co-scheduling...
        * App
* The API and API Server

