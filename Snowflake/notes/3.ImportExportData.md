# Importing and Exporting Data

* File Formats
  * Delimited, JSON, Avro, ORC, Parquet, XML ...
  * Options
    * Compression: pre-compressed or auto compress
    * Encryption: auto compress by snowflake managed keys, or use your own key (business critical)
  * Best Pratices
    * Dedicated data-loading virtual warehouse
    * 10-100 MB compressed single file
    * seperate in folder and mark data info
* Import Tools
  * Web portal
    * select vitual warehouse
    * select file from local computer or stage
    * create/select format
    * other options, what if error happens, rollback or continue with valid data
  * copy cmd
    * `copy into users from @~/staged file_format=( TYPE='CSV' FIELD_DELIMITER=',' SKIP_HEADER=1) ON_ERROR=CONTINUE;`
  * Snow pipe
    * define a pipe

        ```sql
        create or replace pipe <pipe_name> as 
        copy into <table> from <stage> file_format=<defined format>
        ```

    * `select system$pipe_status('<pipe_name>');`
    * trigger immediate refresh: `alter pipe <pipe_name> refresh`
* Data Stages
  * Concept: Virtual file system location. 文件和table data的中转站
  * Types:
    * Internal
      * User Stage(@~): private
      * Table Stage(@%tableName): only for this table
      * Named Stage(@shared): any user any table
    * External: AWS S3, Azure Data Lake, Google Cloud Storage

        ```sql
        create stage `REVIEWS`.`PUBLIC`.AzureDataLake 
        URL = 'azure://snowflakecourse.blob.core.window.net/snowflake-stage'
        CREDENTIALS = (AZURE_SAS_TOKEN = '****************')

        show stages;

        list @azuredatalake
        ```

* Import process
  * select a stage
  * put the file
  * copy into table
  * purge (manually/automatically)
* Export Tools
  * web portal
  * copy command (copy from table to stage)

    ```sql
    copy into <stage>/xxx.parquet
    from (select xxx from xxx)
    file_format=(type=parquet) single=true;
    ```

  * Snowsql (csv file to client computer)
